---
title: "Small Example Workflow"
author: "Iris Foxfoot"
date: "4/5/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```

```{r get_packages}
library(sf) #for polygons
library(tidyverse) #for data wrangling
library(tmap) #for interactive maps
library(janitor) #for cleaning names
library(reshape2) #for melting and recasting data frames (long to wide format)
```

```{r}
#create list of years
kern_field_years <- c(2016:2019)

#loop through read-in process
for (i in kern_field_years) {
  filename <- paste0("kernfields_", i)
  wd <- paste0("data/kernfields/kern", i, ".shp")
  assign(filename, read_sf(wd))
}

#select region (based off of farmer)
kern_subset <- kernfields_2019 %>% 
  filter(AGENT == "McMANUS/WILSON,MICHELE/AARIN")

#crop several other years
kern_2017_crop <- st_crop(kernfields_2017, kern_subset)

kern_2018_crop <- st_crop(kernfields_2018, kern_subset)

kern_2019_crop <- st_crop(kernfields_2019, kern_subset)

#create list of cropped fields and tidy
kern_cropped_list <- lapply(mget(sprintf("kern_%d_crop", 2017:2019)), 
                 function(x) 
                   {
                   clean_names(x) %>%
                     st_make_valid()
                     }
                 )

#bind cropped data together, include new column "source" saying what dataset it came from
sample_bind <- do.call(rbind, lapply(names(kern_cropped_list), 
                                    function(x) 
                                      cbind(kern_cropped_list[[x]], 
                                            source = x)
                                    )) %>% 
    #turn source column into year column by keeping only numbers
    mutate(year = gsub("\\D", "", source)) %>% 
    #Create ID column
    mutate(id = row_number()) %>% 
    #merge comm code to pmt_site
    mutate(pmt_site_code = str_c(pmt_site, "-", comm_code)) %>% 
    #merge ID to  pmt_site
    mutate(pmt_site_id = str_c(pmt_site_code, "-", id)) %>% 
    #merge this column with year
    mutate(pmt_site_string = str_c(year, "_", pmt_site_id))



#select only column we need, clean geometry
sample_clean <- sample_bind %>% 
  dplyr::select(pmt_site_string) %>% 
  st_make_valid()
  

#write to shapefile
#st_write(sample_clean, "data/shapefiles_written/sample_clean.shp", append = F)
```

I took the shapefile `sample_clean.shp` and uploaded it to mapshaper's online interface (https://mapshaper.org/). Then I entered in the console `$ mosaic` and downloaded results as a shapefile, as seen in the next code chunk.

This thread could potentially improve workflow https://github.com/mbloch/mapshaper/issues/353 

```{r}
#read in mapshaper output
sample_mosiac <- st_read("data/mapshaper_outputs/sample_clean/sample_clean.shp")

#intersect with field data 
sample_join <- st_intersection(st_buffer(sample_mosiac, dist = -1), sample_clean)

#pivot so each column is a year
sample_pivot <- sample_join %>% 
  separate(pmt_site_string, into = c("year", "pmt_site"), sep = "_") %>% 
  select(-FID) %>% 
  pivot_wider(names_from = year, values_from = pmt_site) %>%
  st_sf() %>% 
  st_buffer(dist = 1) %>%
  mutate(area_sq_ft = st_area(.))

#unnest listed pmt_site so they are each in own column
sample_pivot_sep <- sample_pivot %>% 
  unnest_wider(c(`2017`, `2018`, `2019`), names_sep = "_")

#write to csv
#write_csv(sample_pivot_sep, "data/shapefiles_written/2017_2019_sample.csv")

#write to shp file
#st_write(sample_pivot_sep, "data/shapefiles_written/2017_2019_shapefile.shp")
```

# How to get rid of slivers?

#### Function to test how many true pmt_site_ids are dropped

```{r}
pmt_deleted <- function (pivot_sep_name) {
  #create df of pivot table without geometry
  pivot_sep_no_geo <- pivot_sep_name %>% 
    select(-geometry, -area_sq_ft)
  
  #create function to check if pmt_site is present anywhere in the pivot table
  pmt_present <- function(x) 
    {
    present <- any(pivot_sep_no_geo==x)
    return(present)
  }
  
  #create list of original pmt_sites
  pmt_list <- as.list(sample_bind$pmt_site_id)

  #run through list and check if they are in pivot df
  present_list <- lapply(pmt_list, pmt_present)
  
  #create df of results
  present_df <- as.data.frame(cbind(pmt_list, present_list)) %>% 
    filter(is.na(present_list))

  return(present_df)
  
  }
```


#### Drop fields under 1 acre

```{r}
#get rid of small fields
sample_pivot_over_acre <- sample_pivot %>% 
  filter(area_sq_ft >= units::set_units(43560,"ft^2"))

#unnest listed pmt_site so they are each in own column
sample_pivot_over_acre_sep <- sample_pivot_over_acre %>% 
  unnest_wider(c(`2017`, `2018`, `2019`), names_sep = "_")

#what fields are under one acre?
under_acre <- sample_bind %>%
  st_sf() %>% 
  mutate(area = st_area(.)) %>% 
  filter(area < units::set_units(43560,"ft^2"))

#gets rid of many but not all slivers
#deletes 65 true fields but, there are only 60 true fields that are under one acre. 
#what are the other five?
```

#### Negative buffer

```{r}
#intersect with field data BUT negative buffer field data
neg_sample_join<- st_intersection(st_buffer(sample_mosiac, dist = -20), sample_clean)

#pivot so each column is a year
neg_sample_pivot <- neg_sample_join %>% 
  separate(pmt_site_string, into = c("year", "pmt_site"), sep = "_") %>% 
  select(-FID) %>% 
  pivot_wider(names_from = year, values_from = pmt_site) %>%
  st_sf() %>% 
  filter(!st_is_empty(.)) %>% 
  st_buffer(dist = 1) %>%
  mutate(area_sq_ft = st_area(.))

#unnest listed pmt_site so they are each in own column
neg_sample_pivot_sep <- neg_sample_pivot %>% 
  unnest_wider(c(`2017`, `2018`, `2019`), names_sep = "_")

#is every permit_site represented in the pivot table?

#with a buffer of -10 we miss 16 fields
#with a buffer of -20 we miss 31 fields
#with a buffer of -30 we miss 40 fields
#with a buffer of -50 we miss 76 fields
```

#### Conditional statement

```{r}
#create column indicating if it was duplicated, area column
sample_conditional <- sample_join %>%
  group_by(pmt_site_string) %>% 
  mutate(dup = n()>1) %>% 
  ungroup() %>% 
  mutate(area_sq_ft = st_area(.))

#filter so fields smaller than one acre are only dropped if duplicated
sample_conditional_drop <- sample_conditional %>% 
  filter(!(dup  == "TRUE" & area_sq_ft <= units::set_units(43560,"ft^2")))

#pivot so each column is a year
conditional_sample_pivot <- sample_conditional_drop %>% 
  separate(pmt_site_string, into = c("year", "pmt_site"), sep = "_") %>% 
  select(-FID) %>% 
  pivot_wider(names_from = year, values_from = pmt_site) %>%
  st_sf() %>% 
  filter(!st_is_empty(.)) %>% 
  st_buffer(dist = 1) %>%
  mutate(area_sq_ft = st_area(.))

#unnest listed pmt_site so they are each in own column
conditional_sample_pivot_sep <- conditional_sample_pivot %>% 
  unnest_wider(c(`2017`, `2018`, `2019`), names_sep = "_")

#drops 25
```

# Original Data

```{r, out.width="120%"}
#reference what output should be
tmap_mode("view")

tm_shape(sample_bind) +
  tm_polygons(id = "pmt_site") +
  tm_facets(by = "year", sync = T) +
  tm_layout(panel.labels = c("2017", "2018", "2019"))
```

# PMT_SITE Pivot Table

```{r}
#test output
tmap_mode("view")

tm_shape(sample_pivot) +
  tm_polygons(id = "pmt_site_year") +
  tmap_options(check.and.fix = TRUE)
```

# PMT_SITE Pivot Table (over 1 acre)

```{r}
#test output
tmap_mode("view")

tm_shape(sample_pivot_over_acre) +
  tm_polygons(id = "pmt_site_year") +
  tmap_options(check.and.fix = TRUE)
```

# PMT_SITE Pivot Table (conditionally over 1 acre)

```{r}
#test output
tmap_mode("view")

tm_shape(conditional_sample_pivot) +
  tm_polygons(id = "pmt_site_year") +
  tmap_options(check.and.fix = TRUE)
```

# PMT_SITE Pivot Table (negative 20 ft buffer)

```{r}
#test output
tmap_mode("view")

tm_shape(neg_sample_pivot) +
  tm_polygons(id = "pmt_site_year") +
  tmap_options(check.and.fix = TRUE)
```

# Creating table with max length of fallowed streak and year the streak ended

```{r}
#identifying fallowed fields function
years <- c(2017, 2018, 2019)

#create dataset for for for loop to loop over
fallowed_sample_pivot <- sample_pivot

#for loop that creates a new column per each year, indicating if field was fallowed or not
for (i in years) {
fallowed_sample_pivot <- fallowed_sample_pivot %>%
  mutate(!!paste0("fallowed_", i) 
         := case_when(!!as.name(i) == "NULL" ~ "fallowed",
                      str_detect(!!as.name(i), "-66000-") & 
                        str_detect(!!as.name(i), ", ", negate = T) ~ "fallowed",
                            T ~ "active"))
}

#create a unique id column from row numbers
fallowed_sample_pivot <- fallowed_sample_pivot %>% 
  mutate(fallow_count_id = row_number())

#drop geometry
fallowed_sample_no_geo <- fallowed_sample_pivot %>% 
  st_drop_geometry()

#transform wide data to the long format
sample_long = melt(fallowed_sample_no_geo, id.vars = c("2017", "2018", "2019", "area_sq_ft", "fallow_count_id")) 

#write new column names
colnames(sample_long) = c("2017", "2018", "2019", "area_sq_ft", "fallow_count_id", "year", "status")

#calculate how long each field has been fallowed by countying repeated fallowed status within rows
sample_long_mutate = sample_long %>%
  #created p-columns that correspond to year
  mutate(p_var = paste0("years_fallowed_", substr(year, 10, 14))) %>% 
  group_by(fallow_count_id) %>%
  #create column that counts how many active or fallowed are in a row
  mutate(lapsed_num = sequence(rle(status)[["lengths"]]),
         final = ifelse(status == "active", 0, 
                           ifelse(str_detect(status, "fallowed"), 
                                  lapsed_num, NA))) %>%
  select(-lapsed_num) %>% 
  data_frame()

#convert if from long format to wide format #NOTE: col names need to be changed when more years are added
df_p = dcast(sample_long_mutate[, c(1:7, 8:9)], fallow_count_id  ~ p_var,  value.var = "final")

#merge it with original data
years_fallowed = merge(df_p, fallowed_sample_pivot, id = fallow_count_id) %>%
  select(-c(fallowed_2017:fallowed_2019)) %>% 
  st_sf()

#create df of only fallow numbers
longest_fallow <- years_fallowed %>%
  select(years_fallowed_2017:years_fallowed_2019) %>% 
  st_drop_geometry()

#create a column indicating where the fallow streak ends (based on max value)
longest_fallow$end_fallow_streak<-colnames(longest_fallow)[apply(longest_fallow, 1, which.max)]

#calculate how long the fallowed streak was
longest_fallow <- longest_fallow %>% 
  mutate(max_years_fallow = pmax(years_fallowed_2017, years_fallowed_2018, years_fallowed_2019)) %>% 
  mutate(end_fallow_streak = str_extract(end_fallow_streak, "\\d+")) %>% 
  select(max_years_fallow, end_fallow_streak)
  
#bind these new columns with orginal data
years_fallowed_clean <- cbind(sample_pivot_sep, longest_fallow)
```

Iris use this thread

https://stackoverflow.com/questions/39575664/r-for-loop-creating-new-columns-populated-by-conditional-statement-based-on-the 


```{r}
#tells me what year the last active crops where
last_crop_df<- years_fallowed %>%
  mutate(`2016` = "outside_time_range") %>% 
  mutate(last_crop_2019 =
           case_when(fallowed_2019 == "fallowed" 
                                   ~ as.character(eval(2019 - p2019)),
                                   T ~ "2019")) %>% 
  mutate(last_crop_final = "find_me")


for(i in 2017:2019) {
  final_last_crop <- last_crop_df %>%
    mutate(last_crop_final = case_when(last_crop_2019 == as.character(i) ~ as.character(!!as.name(i)),
                                T ~ last_crop_final))
}

#replaces year with last active crop
last_crop_mutate <- last_crop_df%>%
  mutate(last_crop_2019 = case_when(last_crop_2019 == "2016" ~ as.character(`2016`),
                                    last_crop_2019 == "2017" ~ as.character(`2017`),
                                    last_crop_2019 == "2018" ~ as.character(`2018`),
                                    T ~ as.character(`2019`)))

#cleaning up cols
last_crop_clean <- last_crop_mutate %>% 
  mutate(years_fallowed_2019 = 
         case_when(fallowed_2019 == "not_fallowed" ~ "0",
                  T ~ as.character(p2019))) %>% 
  select(-c(fallowed_2017:fallowed_2019, p2017:p2019, `2016`, fallow_count_id))

  

```


